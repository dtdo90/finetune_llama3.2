{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b32d521-09e7-4118-8215-b878bcdd24e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:15:13.512912Z",
     "iopub.status.busy": "2025-01-09T02:15:13.512437Z",
     "iopub.status.idle": "2025-01-09T02:15:21.377032Z",
     "shell.execute_reply": "2025-01-09T02:15:21.376428Z",
     "shell.execute_reply.started": "2025-01-09T02:15:13.512890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.2.1)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.2.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.47.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=0.34.0->trl) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.1.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.9.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.17.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.9.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2020.6.20)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets>=2.21.0->trl) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.6.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from peft) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.1.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.47.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.2.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.21.0->peft) (0.27.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers->peft) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->peft) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers->peft) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install trl\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7fc321a-000b-40ec-8b32-c563a9284b06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:15:21.380476Z",
     "iopub.status.busy": "2025-01-09T02:15:21.380288Z",
     "iopub.status.idle": "2025-01-09T02:15:21.685123Z",
     "shell.execute_reply": "2025-01-09T02:15:21.684680Z",
     "shell.execute_reply.started": "2025-01-09T02:15:21.380472Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    access_token = config[\"HF_ACCESS_TOKEN\"]\n",
    "\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe92c1c5-2a0d-4d6e-af8e-f765ada9f6d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:15:21.691208Z",
     "iopub.status.busy": "2025-01-09T02:15:21.687983Z",
     "iopub.status.idle": "2025-01-09T02:15:29.445842Z",
     "shell.execute_reply": "2025-01-09T02:15:29.445314Z",
     "shell.execute_reply.started": "2025-01-09T02:15:21.691184Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets.arrow_dataset import Dataset\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "script_args = Namespace(\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    weight_decay=0.01,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    lora_r=8,\n",
    "    max_seq_length=256,\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    tokenizer_path=\"tokenizer.model\",\n",
    "    # dataset_name=\"tatsu-lab/alpaca\",\n",
    "    dataset_name=\"instruction-data.json\",\n",
    "    device_map=\"cuda\",\n",
    "    use_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    num_train_epochs=10,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=200,\n",
    "    warmup_steps=50,\n",
    "    group_by_length=True, # Group sequences into batches with same length\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    logging_steps=10, # Log every X updates steps\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./results_packing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af546e6c-d5c6-4960-8b4d-e5a9d3fdcd85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:15:29.447195Z",
     "iopub.status.busy": "2025-01-09T02:15:29.446946Z",
     "iopub.status.idle": "2025-01-09T02:15:29.491624Z",
     "shell.execute_reply": "2025-01-09T02:15:29.491185Z",
     "shell.execute_reply.started": "2025-01-09T02:15:29.447169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to instruction-data-train.json\n",
      "Validation data saved to instruction-data-val.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def split_data(args):\n",
    "    \"\"\" Split the saved data into train and val\"\"\"\n",
    "    with open(args.dataset_name,\"r\") as f:\n",
    "        ds=json.load(f)\n",
    "    train_split=int(0.9*len(ds))\n",
    "    ds_train=ds[:train_split]\n",
    "    ds_val=ds[train_split:]\n",
    "\n",
    "    # write into json file\n",
    "    train_file=\"instruction-data-train.json\"\n",
    "    val_file=\"instruction-data-val.json\"\n",
    "\n",
    "    with open(train_file, \"w\") as train_f:\n",
    "        json.dump(ds_train, train_f, indent=4)\n",
    "    print(f\"Training data saved to {train_file}\")\n",
    "\n",
    "    # write validation data to a JSON file\n",
    "    with open(val_file, \"w\") as val_f:\n",
    "        json.dump(ds_val, val_f, indent=4)\n",
    "    print(f\"Validation data saved to {val_file}\")\n",
    "\n",
    "split_data(script_args)\n",
    "\n",
    "def gen_train_input():\n",
    "    \"\"\" Format all data input in alpaca style\n",
    "        Return: a data object which can be accessed via for loop\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    data_file=\"instruction-data-train.json\"\n",
    "    with open(data_file,\"r\") as f:\n",
    "        ds= json.load(f)\n",
    "\n",
    "    for sample in iter(ds):\n",
    "        # extract instruction, input and output text\n",
    "        instruction=sample['instruction']\n",
    "        input_text=sample['input']\n",
    "        output_text=sample['output']\n",
    "        formatted_prompt=None\n",
    "\n",
    "        if input_text is None or input_text == \"\":\n",
    "            formatted_prompt=(\n",
    "                f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "                f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "                f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" # format to signal the model's response\n",
    "                f\"{output_text}<|eot_id|><|end_of_text|>\"\n",
    "            )\n",
    "        else:\n",
    "            formatted_prompt=(\n",
    "                f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "                f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n\"\n",
    "                f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                f\"{output_text}<|eot_id|><|end_of_text|>\"\n",
    "            )\n",
    "        formatted_prompt=\"\".join(formatted_prompt) # exclude trailing white spaces\n",
    "        yield {'text': formatted_prompt}           # stream text into the dataloader, one by one\n",
    "\n",
    "        \n",
    "def gen_val_input():\n",
    "    \"\"\" Format all data input in alpaca style\n",
    "        Return: a data object which can be accessed via for loop\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    data_file=\"instruction-data-val.json\"\n",
    "    with open(data_file,\"r\") as f:\n",
    "        ds= json.load(f)\n",
    "\n",
    "    for sample in iter(ds):\n",
    "        # extract instruction, input and output text\n",
    "        instruction=sample['instruction']\n",
    "        input_text=sample['input']\n",
    "        output_text=sample['output']\n",
    "        formatted_prompt=None\n",
    "\n",
    "        if input_text is None or input_text == \"\":\n",
    "            formatted_prompt=(\n",
    "                f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "                f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "                f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" # format to signal the model's response\n",
    "                f\"{output_text}<|eot_id|><|end_of_text>|\"\n",
    "            )\n",
    "        else:\n",
    "            formatted_prompt=(\n",
    "                f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "                f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n\"\n",
    "                f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                f\"{output_text}<|eot_id|><|end_of_text|>\"\n",
    "            )\n",
    "        formatted_prompt=\"\".join(formatted_prompt) # exclude trailing white spaces\n",
    "        yield {'text': formatted_prompt} # stream text into the dataloader, one by one\n",
    "        \n",
    "train_gen = Dataset.from_generator(gen_train_input)\n",
    "val_gen=Dataset.from_generator(gen_val_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5585cafa-ffac-4701-8ba0-35a4aefd4b94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:15:29.492427Z",
     "iopub.status.busy": "2025-01-09T02:15:29.492267Z",
     "iopub.status.idle": "2025-01-09T02:15:34.009275Z",
     "shell.execute_reply": "2025-01-09T02:15:34.008679Z",
     "shell.execute_reply.started": "2025-01-09T02:15:29.492411Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_and_prepare_model(args):\n",
    "    model=AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name,\n",
    "        # quantization_config=args.bnb_config,\n",
    "        device_map=args.device_map,\n",
    "        token=True\n",
    "    )\n",
    "\n",
    "    peft_config=LoraConfig(\n",
    "        lora_alpha=script_args.lora_alpha,\n",
    "        lora_dropout=script_args.lora_dropout,\n",
    "        r=script_args.lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj'],\n",
    "        # target_modules=[\"query_key_value\"]\n",
    "    )\n",
    "    tokenizer=AutoTokenizer.from_pretrained(script_args.model_name, trust_remote_code=True)\n",
    "    tokenizer.add_special_tokens\n",
    "    tokenizer.pad_token=\"<|end_of_text|>\" # this token is already available in tokenizer list\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return model,peft_config,tokenizer\n",
    "\n",
    "model,peft_config,tokenizer=create_and_prepare_model(script_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c6f6c0f-d644-4d7c-9027-36e11dd0304a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:15:35.680053Z",
     "iopub.status.busy": "2025-01-09T02:15:35.679562Z",
     "iopub.status.idle": "2025-01-09T02:17:08.037121Z",
     "shell.execute_reply": "2025-01-09T02:17:08.036618Z",
     "shell.execute_reply.started": "2025-01-09T02:15:35.680023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e08df86345a4fcba6bbae5f6a45f43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-09 02:15:36,114] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:29, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.330800</td>\n",
       "      <td>3.688517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.069100</td>\n",
       "      <td>3.073312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.165100</td>\n",
       "      <td>2.045742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.165400</td>\n",
       "      <td>1.425161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>1.313111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.745500</td>\n",
       "      <td>1.320160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.730700</td>\n",
       "      <td>1.308774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.725300</td>\n",
       "      <td>1.303558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.666600</td>\n",
       "      <td>1.297198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.671900</td>\n",
       "      <td>1.278009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.703500</td>\n",
       "      <td>1.263294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.660400</td>\n",
       "      <td>1.270353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.628400</td>\n",
       "      <td>1.274776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.657100</td>\n",
       "      <td>1.288972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>1.294213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.629700</td>\n",
       "      <td>1.283713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.634800</td>\n",
       "      <td>1.284479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.624100</td>\n",
       "      <td>1.289135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.604100</td>\n",
       "      <td>1.291605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.595400</td>\n",
       "      <td>1.292594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=1.023242733478546, metrics={'train_runtime': 91.3593, 'train_samples_per_second': 70.053, 'train_steps_per_second': 2.189, 'total_flos': 2323685094236160.0, 'train_loss': 1.023242733478546, 'epoch': 6.451612903225806})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_arguments=TrainingArguments(\n",
    "    output_dir=script_args.output_dir,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    optim=script_args.optim,\n",
    "    save_steps=script_args.save_steps,\n",
    "    logging_steps=script_args.logging_steps,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    fp16=script_args.fp16,\n",
    "    bf16=script_args.bf16,\n",
    "    max_grad_norm=script_args.max_grad_norm,\n",
    "    max_steps=script_args.max_steps,\n",
    "    warmup_steps=script_args.warmup_steps,\n",
    "    group_by_length=script_args.group_by_length,\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "    report_to=\"none\", # prevent error with wandb\n",
    "    eval_strategy=\"steps\",  # Evaluate periodically\n",
    "    eval_steps=script_args.eval_steps,  # Perform evaluation every X steps\n",
    ")\n",
    "\n",
    "\n",
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_gen,\n",
    "    eval_dataset=val_gen,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e10e0-28bb-46cd-97ca-0fdd575d96db",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "163aed01-1b9e-4fae-a986-0cb820db3c6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:23:25.707529Z",
     "iopub.status.busy": "2025-01-09T02:23:25.707293Z",
     "iopub.status.idle": "2025-01-09T02:23:25.713109Z",
     "shell.execute_reply": "2025-01-09T02:23:25.712543Z",
     "shell.execute_reply.started": "2025-01-09T02:23:25.707480Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(model, prompt, tokenizer, max_new_tokens, context_size=256, temperature=0.0, top_k=1, eos_id=[128001,128009]):\n",
    "    \"\"\" Generate till reaching max_new_tokens or till eos_id=<|end_of_text|>\"\"\"\n",
    "    # format prompt\n",
    "    # formatted_prompt=(\n",
    "    #     f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    #     f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    #     f\"### Instruction:\\n{prompt}\"\n",
    "    # )\n",
    "    formatted_prompt=(\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "        f\"### Instruction:\\n{prompt}\"\n",
    "        f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "    idx=tokenizer.encode(formatted_prompt)\n",
    "    idx=torch.tensor(idx).unsqueeze(0).to(script_args.device_map) # add batch dimension\n",
    "    _,num_tokens=idx.shape\n",
    "    #print(\"Number of input tokens: \",num_tokens)\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            outputs = model.forward(idx_cond)\n",
    "            logits=outputs.logits\n",
    "        # last time step\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next in eos_id:  # Stop generating early if <|eot_id|> or<|end_of_text|> token is encountered \n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "        \n",
    "    #print(f\"Output tensor: {idx.shape}\")\n",
    "    # remove batch dimension\n",
    "    idx_flat=idx.squeeze(0)\n",
    "    generated_ids=idx_flat[num_tokens:] # take out the input prompt\n",
    "    generated_text=tokenizer.decode(generated_ids)\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da3888c9-9e84-4f1d-9b81-01f78b0fce36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:23:41.504070Z",
     "iopub.status.busy": "2025-01-09T02:23:41.503866Z",
     "iopub.status.idle": "2025-01-09T02:23:42.492342Z",
     "shell.execute_reply": "2025-01-09T02:23:42.491938Z",
     "shell.execute_reply.started": "2025-01-09T02:23:41.504054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The human heart is a muscular organ that pumps blood throughout the body, supplying oxygen and nutrients to tissues and organs. It also helps to regulate blood pressure and maintain blood flow to the brain and other vital organs.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=\"explain the function of human heart\"\n",
    "generate(model, prompt, tokenizer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aedf79d-e9ad-4c9e-b31a-d7d3853c6df7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:50:20.221903Z",
     "iopub.status.busy": "2025-01-09T02:50:20.221696Z",
     "iopub.status.idle": "2025-01-09T02:50:26.240484Z",
     "shell.execute_reply": "2025-01-09T02:50:26.239854Z",
     "shell.execute_reply.started": "2025-01-09T02:50:20.221888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as LLAMA32_fine_tuned.pth\n"
     ]
    }
   ],
   "source": [
    "# save model dict\n",
    "model_file_name=\"LLAMA32_fine_tuned.pth\"\n",
    "torch.save(model.state_dict(), model_file_name)\n",
    "print(f\"Model saved as {model_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78842876-ba0d-4c63-822d-238614aa06a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:24:56.690344Z",
     "iopub.status.busy": "2025-01-09T02:24:56.690130Z",
     "iopub.status.idle": "2025-01-09T02:25:32.270523Z",
     "shell.execute_reply": "2025-01-09T02:25:32.269862Z",
     "shell.execute_reply.started": "2025-01-09T02:24:56.690328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:35<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response saved as test-data-with-response.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# load test data\n",
    "test_data_path=\"instruction-data-val.json\"\n",
    "with open(test_data_path,\"r\") as f:\n",
    "    test_data=json.load(f)\n",
    "\n",
    "for i,entry in tqdm(enumerate(test_data),total=len(test_data)):\n",
    "    generated_text=generate(model, entry[\"instruction\"], tokenizer, max_new_tokens=100)\n",
    "    test_data[i][\"model response\"]=generated_text\n",
    "\n",
    "# write into a file\n",
    "test_data_path=\"test-data-with-response.json\"\n",
    "\n",
    "with open(test_data_path,\"w\") as file:\n",
    "    json.dump(test_data,file, indent=4)\n",
    "print(f\"Response saved as {test_data_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d503f7c2-4bd0-498f-a328-502334c7289c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:47:06.015591Z",
     "iopub.status.busy": "2025-01-09T02:47:06.015126Z",
     "iopub.status.idle": "2025-01-09T02:47:06.019093Z",
     "shell.execute_reply": "2025-01-09T02:47:06.018570Z",
     "shell.execute_reply.started": "2025-01-09T02:47:06.015574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/notebooks/Llama32-finetune/LLAMA32_fine_tuned.pt' target='_blank'>/notebooks/Llama32-finetune/LLAMA32_fine_tuned.pt</a><br>"
      ],
      "text/plain": [
       "/notebooks/Llama32-finetune/LLAMA32_fine_tuned.pt"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download file\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Replace 'filename.ext' with your file name\n",
    "FileLink(\"/notebooks/Llama32-finetune/LLAMA32_fine_tuned.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c7282",
   "metadata": {},
   "source": [
    "## Load fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ad2e0-6845-49bf-a7b4-c52573faf451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "state_dict=torch.load(\"LLAMA32_fine_tuned.pth\", map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_fine_tune_model(base_model_id, saved_weights,args):\n",
    "    \"\"\" Load the fine tuned model and tokenizer\n",
    "        The fined model is LLAMA3.2 + LoRA layers. We use PeftModel to load it\n",
    "    \"\"\"\n",
    "    # load base model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_id,device_map=args.device_map)\n",
    "\n",
    "\n",
    "    # Load LoRA model\n",
    "    peft_config=LoraConfig(\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        r=args.lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj'],)\n",
    "    lora_model = PeftModel(base_model, peft_config)\n",
    "\n",
    "    # Load and adapt state dict\n",
    "    state_dict = torch.load(saved_weights)\n",
    "    \n",
    "    # Adapt keys to match expected format\n",
    "    #   Expected keys have prefix: base_model.model.model.\n",
    "    #   Saved weights have prefix: model.\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        # Add the expected prefix\n",
    "        new_key = f\"base_model.model.{key}\"\n",
    "        new_state_dict[new_key] = value\n",
    "\n",
    "    # load finetuned lora weights\n",
    "    lora_model.load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    # Ensure model is in eval mode for inference\n",
    "    lora_model = lora_model.eval()\n",
    "    \n",
    "    # Clear CUDA cache to free up memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return lora_model, tokenizer\n",
    "\n",
    "base_model_id=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "lora_weights=\"LLAMA32_fine_tuned.pth\"\n",
    "model_ft,tokenizer=load_fine_tune_model(base_model_id,lora_weights,script_args)\n",
    "\n",
    "# Generate with ft model: adjust temperature and top_k for different outputs\n",
    "prompt=\"Explain the function of human heart\"\n",
    "print(generate(model_ft, prompt, tokenizer, max_new_tokens=100,temperature=0.8,top_k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0511c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN_M1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
